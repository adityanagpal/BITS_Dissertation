# -*- coding: utf-8 -*-
"""BITS_Dissertation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_gy0Izuecb8H6bIfTXbXeDS3oM8qYaPh
"""

!pip install mediapipe

import mediapipe as mp
mp_face_detection = mp.solutions.face_detection
mp_face_mesh = mp.solutions.face_mesh
mp_objectron = mp.solutions.objectron
mp_selfie_segmentation = mp.solutions.selfie_segmentation

mp_drawing = mp.solutions.drawing_utils 
drawing_spec = mp_drawing.DrawingSpec(thickness=3, circle_radius=2)
mp_drawing_styles = mp.solutions.drawing_styles

from google.colab import drive
drive.mount('/content/drive/',force_remount=True)

import os

# Used: 0, 40, 39, 8, 4, 15

vid_name = os.listdir('drive/MyDrive/Test/Video')[::-1][4]
vid_name

!rm -rf /content/frames
!mkdir /content/frames
import cv2
import numpy as np
from PIL import Image
from glob import glob

vidcap = cv2.VideoCapture(f"/content/drive/MyDrive/Test/Video/{vid_name}")
success,image = vidcap.read()
count = 0


while success:
  cv2.imwrite("/content/frames/frame%05d.png" % count, image)
  success,image = vidcap.read()
  count += 1
  if count == 20:
    break

from natsort import natsorted
from glob import glob

file_list = glob('/content/frames/*.png')
file_list_sorted = natsorted(file_list, reverse=False)

# Run MediaPipe Face Detection with short range model.
import cv2
from google.colab.patches import cv2_imshow
import math
import numpy as np
from PIL import Image

DESIRED_HEIGHT = 480
DESIRED_WIDTH = 480

def resize_and_show(image):
  h, w = image.shape[:2]
  if h < w:
    img = cv2.resize(image, (DESIRED_WIDTH, math.floor(h/(w/DESIRED_WIDTH))))
  else:
    img = cv2.resize(image, (math.floor(w/(h/DESIRED_HEIGHT)), DESIRED_HEIGHT))
  cv2_imshow(img)

def create_collage(name, width, height, listofimages):
    cols = 5
    rows = 2
    thumbnail_width = width//cols
    thumbnail_height = height//rows
    size = thumbnail_width, thumbnail_height
    new_im = Image.new('RGB', (width, height))
    ims = []
    for p in listofimages:
        im = Image.open(p)
        im.thumbnail(size)
        ims.append(im)
    i = 0
    x = 0
    y = 0
    for col in range(cols):
        for row in range(rows):
            print(i, x, y)
            new_im.paste(ims[i], (x, y))
            i += 1
            y += thumbnail_height
        x += thumbnail_width
        y = 0

    new_im.save(f"{name}.jpg")

!mkdir /content/fd

with mp_face_detection.FaceDetection(
    min_detection_confidence=0.5, model_selection=0) as face_detection:
  for name in file_list_sorted[:10]:
    image = cv2.imread(name)
    
    # Convert the BGR image to RGB and process it with MediaPipe Face Detection.
    results = face_detection.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))

    # Draw face detections of each face.
    print(f'Face detections of {name}:')
    if not results.detections:
      continue
    annotated_image = image.copy()
    resize_and_show(annotated_image)
    for detection in results.detections:
      mp_drawing.draw_detection(annotated_image, detection)
    print("==>", name.split('/')[-1])
    cv2.imwrite(f"/content/fd/{name.split('/')[-1]}", annotated_image)
    resize_and_show(annotated_image)

!mkdir /content/fm

# Run MediaPipe Face Mesh.
with mp_face_mesh.FaceMesh(
    static_image_mode=True,
    refine_landmarks=True,
    max_num_faces=2,
    min_detection_confidence=0.5) as face_mesh:
  for name in file_list_sorted[:10]:
    image = cv2.imread(name)
    # Convert the BGR image to RGB and process it with MediaPipe Face Mesh.
    results = face_mesh.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))

    # Draw face landmarks of each face.
    print(f'Face landmarks of {name}:')
    if not results.multi_face_landmarks:
      continue
    annotated_image = image.copy()
    for face_landmarks in results.multi_face_landmarks:
      mp_drawing.draw_landmarks(
          image=annotated_image,
          landmark_list=face_landmarks,
          connections=mp_face_mesh.FACEMESH_TESSELATION,
          landmark_drawing_spec=None,
          connection_drawing_spec=mp_drawing_styles
          .get_default_face_mesh_tesselation_style())
      mp_drawing.draw_landmarks(
          image=annotated_image,
          landmark_list=face_landmarks,
          connections=mp_face_mesh.FACEMESH_CONTOURS,
          landmark_drawing_spec=None,
          connection_drawing_spec=mp_drawing_styles
          .get_default_face_mesh_contours_style())
      mp_drawing.draw_landmarks(
          image=annotated_image,
          landmark_list=face_landmarks,
          connections=mp_face_mesh.FACEMESH_IRISES,
          landmark_drawing_spec=None,
          connection_drawing_spec=mp_drawing_styles
          .get_default_face_mesh_iris_connections_style())
    cv2.imwrite(f"/content/fm/{name.split('/')[-1]}", annotated_image)
    resize_and_show(annotated_image)

!mkdir /content/fo

with mp_objectron.Objectron(
    static_image_mode=True,
    max_num_objects=5,
    min_detection_confidence=0.5,
    model_name='Shoe') as objectron:
  # Run inference on shoe images.
  for name in file_list_sorted[:10]:
    image = cv2.imread(name)
    # Convert the BGR image to RGB and process it with MediaPipe Objectron.
    results = objectron.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))

    # Draw box landmarks.
    if not results.detected_objects:
      print(f'No box landmarks detected on {name}')
      continue
    print(f'Box landmarks of {name}:')
    annotated_image = image.copy()
    for detected_object in results.detected_objects:
      mp_drawing.draw_landmarks(
          annotated_image, detected_object.landmarks_2d, mp_objectron.BOX_CONNECTIONS)
      mp_drawing.draw_axis(annotated_image, detected_object.rotation, detected_object.translation)
    cv2.imwrite(f"/content/fo/{name.split('/')[-1]}", annotated_image)
    cv2_imshow(annotated_image)

!mkdir /content/fseg

# Show segmentation masks.
BG_COLOR = (192, 192, 192) # gray
MASK_COLOR = (255, 255, 255) # white

with mp_selfie_segmentation.SelfieSegmentation() as selfie_segmentation:
  for name in file_list_sorted[:10]:
    image = cv2.imread(name)
    # Convert the BGR image to RGB and process it with MediaPipe Selfie Segmentation.
    results = selfie_segmentation.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
    
    # Generate solid color images for showing the output selfie segmentation mask.
    fg_image = np.zeros(image.shape, dtype=np.uint8)
    fg_image[:] = MASK_COLOR
    bg_image = np.zeros(image.shape, dtype=np.uint8)
    bg_image[:] = BG_COLOR
    condition = np.stack((results.segmentation_mask,) * 3, axis=-1) > 0.2
    output_image = np.where(condition, fg_image, bg_image)

    print(f'Segmentation mask of {name}:')
    cv2.imwrite(f"/content/fseg/{name.split('/')[-1]}", output_image)
    resize_and_show(output_image)

from glob import glob
create_collage("face-detection", 800, 300, glob("/content/fd/*.png"))
Image.open("/content/face-detection.jpg")

from glob import glob
create_collage("face-mesh", 800, 300, glob("/content/fm/*.png"))
Image.open("/content/face-mesh.jpg")

from glob import glob
create_collage("face-seg", 800, 300, glob("/content/fseg/*.png"))
Image.open("/content/face-seg.jpg")

import torch.nn as nn

class MLP(nn.Module):
    def __init__(self):
        super(MLP, self).__init__()
        self.fc1 = nn.Linear(1693, 256) # First fully connected layer
        self.fc_hidden = nn.ModuleList([nn.Linear(256, 256) for i in range(16)]) # Hidden layers
        self.fc_out = nn.Linear(256, 5) # Output layer
        self.softmax = nn.Softmax(dim=1) # Softmax layer for classification

    def forward(self, x1, x2, x3):
        x = torch.cat((x1, x2, x3), 1)
        x = x.view(-1, 1693) # Reshape input to be 1x64
        x = nn.functional.relu(self.fc1(x)) # Apply ReLU activation function to first layer
        for fc in self.fc_hidden:
            x = nn.functional.relu(fc(x)) # Apply ReLU activation function to each hidden layer
        x = self.fc_out(x) # Output layer
        x = self.softmax(x) # Softmax layer for classification
        return x

model = MLP()

import torch

# focused / distracted / thinking / neutral / ignoring

inp1 = torch.from_numpy(fd_array)[0]
inp2 = torch.from_numpy(fm_array)[0]
inp3 = torch.from_numpy(sg_array)[0]

# print(inp1.unsqueeze(0).shape, inp2.unsqueeze(0).shape, inp3.unsqueeze(0).shape, inp1.dtype)

model(inp1.unsqueeze(0), inp2.unsqueeze(0), inp3.unsqueeze(0)).shape

